<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Hierarchical clustering </title>
       <style type="text/css">
        body {
            font-family: verdana, arial, sans-serif;
        }
        b {
            font-size: 125%;
            color: blue;
        }
        i {
            font-size: 110%;
            color: green;
        }
    </style>
</head>
<body>

<div><p>
    На этом занятии мы применим алгоритм иерархической кластеризации к небольшому объему
    данных, вытащим из этой иерархии кластеры и попытаемся их проинтерпретировать. У меня
    есть небольшой датасет, в котором объектами являются продукты питания, а их признаками
    являются, во-первых, энергетическая ценность в килокалориях, количество жиров и
    протеинов в граммах, количество кальция и железа в миллиграммах. Первое, что стоит
    заметить — это то, что все признаки в этом датасете имеют разную шкалу, причем
    наибольшую шкалу имеет признак именно энергетической ценности, его шкала на порядок
    больше, чем шкалы всех остальных признаков. Это значит, что когда мы будем считать
    попарное расстояние между объектами, то наибольший вклад в это расстояние будет вносить
    признак «энергетическая ценность». И нас это не интересует, мы хотим, чтобы все признаки
    вносили одинаковый вклад в расстояние между объектами. Есть несколько способов привести
    признаки к единой шкале. Один из таких способов — это вычесть из каждого признака его
    среднее значение и затем поделить на стандартное отклонение, что я и сделаю. Для начала
    я перенесу все мои объекты с признаками в некоторую матрицу X. Далее я вычту из этой
    матрицы средние значения по всем признакам и поделю на стандартные отклонения тоже по
    всем признакам отдельно. Давайте проверим, что все правильно. Посчитаем среднее значение
    результирующей матрицы по всем признакам в отдельности. Все средние значения либо очень
    близки к нулю, либо равны нулю. И теперь стандартное отклонение, аналогично по всем
    признакам в отдельности, оно равно единице. Все, теперь мы перешли к одинаковой для
    всех признаков безразмерной шкале, вклад их будет одинаковый в расчет расстояния. Теперь
    можно применять иерархическую кластеризацию. Несмотря на то, что в библиотеке
    scikit-learn есть инструменты для того, чтобы делать агломеративную кластеризацию, я
    буду использовать инструментарий библиотеки SciPy. Для этого из SciPy я импортирую
    следующие методы. Во-первых, это linkage, далее fcluster и dendrogram. Первый метод,
    который нам понадобится — это linkage. На вход ему подаются, во-первых, сами объекты
    матрицы X. Далее идут атрибуты method и metric. В атрибуте method мы указываем то, как
    мы будем пересчитывать расстояние между кластерами после объединения. Я использую
    average linkage. И, наконец, атрибут metric позволяет нам определить то, как мы будем
    считать попарные расстояния между объектами. Я буду использовать вариант по умолчанию,
    то есть евклидову метрику. В результате в переменной Z мы имеем следующую табличку из
    четырех столбцов. Первые два столбца содержат в себе индексы объектов, которые на этом
    шаге будут объединяться. Третий столбец содержит расстояние, на котором эти два объекта
    будут объединяться. И, наконец, четвертый столбец показывает нам, какой размер кластера
    получится после объединения на этом шаге. То есть в данном случае объект 0 и объект 10,
    между которыми расстояние в данный момент равняется примерно 0,07, объединятся в кластер
    размера 2. И таким образом указана информация о всех объединениях в нашем датасете.
    Теперь давайте подадим эту переменную Z в метод dendrogram. В результате мы получим
    визуализацию нашей дендрограммы. По оси x у нас действительно содержится, расположены
    индексы объектов, по оси y — расстояния объединений этих объектов. Можно сделать
    дендрограмму чуть более красивой. Например, можно сделать так, чтобы она шла не снизу
    вверх, а справа налево. Для этого я в атрибут orientation укажу строку left. Дальше
    можно убрать эту раскраску, которая у нас есть по умолчанию. Для этого в color_threshold
    я передам значение 0. И, наконец, вместо того чтобы у нас были какие-то безымянные
    индексы объектов, я хочу видеть именно названия продуктов питания. Для этого в attribute
    labels я добавлю названия продуктов из датафрейма. В результате я получил довольно
    красивую дендрограмму. Теперь у нас названия объектов идут по оси y, а расстояние, на
    котором объекты объединяются — по оси x. Теперь хотелось бы из всей этой иерархии
    получить разбиение на кластеры. Для этого мы будем использовать метод fcluster. Ему мы
    опять же подаем переменную Z, и далее нам нужно указать критерий, по которому мы хотим
    отобрать кластеры. Есть несколько способов это сделать, но наиболее популярный из них —
    это либо в явном виде указать количество кластеров, которое мы хотим получить, либо,
    смотря на дендрограмму, определить порог отсечения, и по этому порогу нам вернутся метки
    тех кластеров, которые еще не успели объединиться вместе. Ну вот, например, выберем
    порог 2,2. И для того чтобы мы использовали именно нужный критерий, мы укажем критерий
    distance. В результате мы получили шесть кластеров. Ну и, действительно, если посмотреть
    на дендрограмму, прикинуть, где находится порог 2,2, и провести вот такую линию, то как
    раз понятно, что это за шесть кластеров у нас получилось. Теперь давайте их все
    проинтерпретируем. Объектов у нас не так много, поэтому мы можем посмотреть на все
    объекты целиком. Для этого напишем такой цикл. Небольшие красивости. И, чтобы он работал,
    в датафрейм исходный мы добавим новый столбец, который назовем label, и передадим туда
    то разбиение, которое мы получили после fcluster. Ну вот мы видим перечисление тех
    объектов, которые попали в тот или иной кластер. Давайте попытаемся их
    проинтерпретировать. Например, в первый кластер попали два объекта — это мидии. Почему
    они выделились в отдельный кластер? Потому что при их низких значениях энергетической
    ценности и концентрациях протеинов и жиров у них довольно высокие концентрации кальция
    и железа. Второй кластер состоит из продуктов питания, богатых калориями и жирами.
    Третий кластер состоит из рыбных продуктов, и они отличны тем, что при не выделяющихся
    энергетической ценности, протеинах, жирах и железе довольно большая концентрация кальция.
    Четвертый кластер не выделяется ничем, это просто продукты питания, у которых довольно
    средние показатели по всем признакам. И, наконец, у нас есть изолированные кластеры пять
    и шесть, которые отличны по какому-то одному из признаков, например, последний объект,
    сардины, имеет очень большое значение в концентрации кальция. Таким образом мы смогли с
    помощью иерархической кластеризации получить кластеры и наглядно их проинтерпретировать.
</p></div>

</body>
</html>