<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Практика. Применение PCA на данных</title>
        <style type="text/css">
        body {
            font-family: verdana, arial, sans-serif;
        }
        b {
            font-size: 125%;
            color: blue;
        }
        i {
            font-size: 110%;
            color: green;
        }
    </style>
</head>
<body>

<div><p>
  На этом занятии мы с вами применим метод главных компонент тремя разными способами и заодно проверим как он влияет на
  качество задачи и классификации. Мы будем использовать dataset про вино. Он состоит из одного целевого признака
  качества и 11 остальных признаков, которые как то характеризуют вино. Мы его перевели из разряда задачи в весе, задачи
  классификации. То есть мы будем с вами отличать вино хорошее от вина плохого по его каким-то характеристикам. Итак,
  первый способ, которым мы будем применять, это коробочный метод, то есть мы возьмем готовое решение из библиотеки
  sklearn. Метод главный компонент у нас лежит в разделе decomposition и тут делается все точно так же, как мы всегда
  раньше делаем, мы берем pca для наглядности я возьму количество компонент, например 6, и дальше используем наши
  любимые команды fit predict pca.fit(X). Для того, чтобы получить уже представление о наших объектов в сжатом признаком
  в пространстве я использую метод Transform. У нас действительно получилось 1600 объектов как было изначально на шесть
  признаков. Что у нас лежит в pca? Но, во-первых это сами коэффициенты, которые необходимо использовать для того, чтобы
  перейти от исходного признакового пространства в сжатые. Размерность этого элемента составляет 6x11, то есть как раз
  те самые коэффициенты, их 11 штук для каждого нового признака, который нужно домножить на исходное значение признаков.
  Давайте попробуем сделать это сами, то есть возьмем матрицу X и домножим на (pac.components_.T), транспонируем, чтобы
  она сошлись размерности. Ну, и на самом деле, если мы сравним то, что мы сейчас получили с тем что получилось после
  трансформа, то результат не сойдется. Почему это так? Потому что pca в себе центрирует признаки. То есть вычитает из
  каждого из них среднее, таким образом чтобы каждая средняя, каждая из них была равно нулю. Но, мы сделаем то же самое
  заранее. Для этого я создал матрицу X_ c нижним подчеркиваем, которую получу путем вычитания из исходной матрицы
  среднего по каждому статусу.
    И теперь уже домножу ее на pca.components. И теперь результат у нас сойдется. Отлично! Так же в pca лежит атрибут
    <b>.exlplained_variance_</b> и <b>.explained_variance_retion_</b>. Вот, как раз доли объяснен дисперсии и в принципе
    дисперсия, в которой выражается каждый из компонент. Мы чуть позже вернемся к этому. Теперь попробуем применить
    метод главного компонент с помощью сингулярного разложение. Сингулярное разложение у нас лежит в модуле numpy и на
    выходе у нас должно быть три матрицы u, s и vt транспонированная. В сингулярное разложение сразу подаю
    центрированную матрицу и ставлю атрибут full_matrices=0. Посмотрим на размерности. Итак, у нас действительно
    некоторая матрица. S - это некоторый вектор размера 11 и vt - это тоже матрица размером 11x11. То есть мы сделали
    полное сингулярное разложение, а нам нужно его укротить. Ну, для начала давайте проверим, что получается
    действительно, то что получается при полном сингулярном разложении, то есть матрицы идеально восстанавливаются. Для
    этого я запишу результат перемножения в матрицу X_svd=u.dot(s).dot(vt). Ну и на самом деле у меня сразу будет
    ошибка. Почему? Потому что это s - это вектор, а я хочу чтобы это была диагональная матрица. То есть я сделаю
    np.diag(s), здесь заменю s маленькое на S большое. И теперь давайте сравним как-то результат восстановления с тем,
    что было изначально.
    Для этого мы просто поэлементно вычтим результат восстановления и изначальную матрицу и возведем это в квадрат,
    посчитаем сумму таких элементов, получается какое-то очень маленькое число. Ok! Вернемся к методу главных компонент.
    Посмотрим, что у нас лежит в матрице vt и сравним это с тем, что лежит в pca.components. Но и на самом деле можно
    невооруженным глазом заметить, что элементы одни и те же, но в некоторых случаях там меняется знак. Но, это ничего
    страшного, это с одной стороны особенность алгоритма, а с другой стороны там получается эквивалентные лещи, если в
    матрице в сингулярном разложении в соответствующую строку в матрице u и столбец в матрицы v домножить на -1, то
    получится одно и то же. Поэтому разницы большой нету. Таким образом мы возьмем vt, возьмем оттуда 6 первых строк,
    потому что, у нас всего 6 компонент было изначально и транспонируем результат. Теперь сделаем преобразование
    <i>Z_svd</i> это у нас X_ с нижним почерком.dot на v. Давайте просто сравним результат.
    Кажется, что получается все одно и то же, опять же с точностью до знака. И напоследок сделаем метод главных
    компонент с помощью собственных чисел и собственных векторов матрицы к-вариаций. Для начала посчитаем саму матрицу
    к-вариаций. Это X_ с нижним подчеркиваем транспонированная на x_ с нижней подчеркиваем. Теперь найдем собственные
    вектора и собственные числа этой матрицы. Для этого мы используем метод <i>eig</i> <b>eigenvectors</b> и
    <b>eigenvelues</b>.
    В lambd у нас лежат собственные числа матрицы к-вариаций, а v - матрица состоящая из собственных векторов. Ну и
    опять же можно посмотреть, что все что в ней лежит очень похоже на то, что у нас получилось в матрице v
    транспонированной и в сингулярном разложении и то, что лежит в pca в атрибуте components. Вернемся к объясненной
    дисперсии. Напомним, что он лежит в pca _explained_variance_ratio и теперь посчитаем все то же самое через
    собственные числа матрицы к-вариации. Для этого, мы возьмем их, как они есть и просто поделим на их сумму общую.
    Ну, и у нас получаются все те же самые результаты что и здесь. Отлично! Также, обратите внимание, что если мы
    нарисуем график, доля объясненной дисперсии с каждой компонентой накопленным, то получится довольно странный
    результат. Оказывается у нас 90% всей дисперсии объясняется уже первой компонентой. Значит ли это, что нам нужно
    использовать только один какой-то признак. Но, это необязательно. Дело в том, что у нас признаки изначально, они в
    разной шкале находятся. Поэтому рекомендуется, кроме центрирования матрицы еще ее нормирует, то есть делить на
    стандартные отклонения. Это мы сделаем дальше. Итак, теперь посмотрим, как влияет метод главных компонент на
    качество классификации. Наш базовый Pipeline будет состоять из двух этапов. Сначала мы признаки центрируем, потом
    применим логистическую регрессию и посмотрим какой получим результат без метода главных компонент. Для этого я
    использую метод cross_val_score_, на вход я подам baseline модель, исходные признаки x, целевую переменную y. В
    scoring я укажу как accuracy и кросс-валидация у меня будет состоять из 5 фолдов. Я уже ее обозначил. И посчитаем
    среднее значение меры качества. Ну, получилось почти 0,75% в точности. Теперь добавим в наш Pipeline метод главных
    компонент. Это довольно просто сделать. То есть теперь наш Pipeline будет выглядеть так. Между шагом шкалирования
    признаков и собственно классификатором. Я добавлю самый главный компонент с некоторым количеством компонент на
    выходе, которые я хочу получить. Вот и посмотрим, как меняется качество модели с разным количеством компонент.
    Например, посмотрим интервал от 1 до 11. Давайте запустим это в цикле. На каждой итерации будем менять количество
    компонент и смотреть на качество cross_val_score. Давайте тоже это будем все запоминать в некоторый список score,
    который я инициализирую чуть выше.
    Но и давайте это нарисуем. plt.plot. K по оси x и score по оси y. Ну, и в качестве референсной точки мы нарисуем,
    то что у нас получилось в качестве начального приближения, base_score назовем это.
    Что мы увидим? Что на самом деле так и бывает на практике, метод главных компонент редко дает какой-то прирост
    качества, но и при этом с небольшим уменьшением качества признаков, качество падает тоже не сильно. Вот, например,
    при количестве главных компонент равной 9 ну или изначально 11 качество, если не лучше то точно такое же как было
    изначально. Ну, а дальше оно падает от 3 до 7 но незначительно. То есть все таки можно еще использовать метод
    главных компонент при не сильном улучшении качества.
</p></div>

</body>
</html>