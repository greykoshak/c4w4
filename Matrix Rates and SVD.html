<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Практика. Матрица рейтингов и SVD</title>
            <style type="text/css">
        body {
            font-family: verdana, arial, sans-serif;
        }
        b {
            font-size: 125%;
            color: blue;
        }
        i {
            font-size: 110%;
            color: green;
        }
    </style>
</head>
<body>

<div><p>
    На этом занятии мы с вами научимся получать матрицу рейтингов из исходных данных, применять метод сингулярного
    разложения к матрице рейтингов и считать похожесть пользователей по матрице рейтингов. Изначально у меня есть две
    таблицы. Первая таблица — это таблица с рейтингами, то есть у нас есть поле «Идентификатор пользователя»,
    «Идентификатор фильма», «Рейтинг» и «Время, когда рейтинг был проставлен». Есть таблица с фильмами, то есть у нас
    есть идентификатор фильма, название фильма и прочая информация про этот фильм. Первое, что стоит заметить, это то,
    как распределены идентификаторы фильмов и пользователей. Вначале, если посмотреть на минимальное и максимальное
    значения, то кажется, что у нас должно быть довольно много пользователей. Но если просто посчитать количество
    уникальных значений, то их оказывается не так много. В дальнейшем для удобства лучше кодировать эти идентификаторы
    числами от нуля до количества уникальных значений, что для пользователя, что для фильмов. Делать мы это будем с
    помощью преобразователя, который называется LabelEncoder, он есть в sklearn. Мы для начала создадим экземпляр класса
    и обучим их. Обучать мы их будем на матрице рейтингов. Преобразователь для пользователей мы обучим на
    соответствующее поле userID.values, для фильмов — на поле movieID.values. И перед как мы сделаем переобозначение,
    еще из матрицы с фильмами удалим те фильмы, которые не встречаются в матрице рейтингов, просто для удобства. Для
    этого мы сделаем проверку, что значение в поле «Идентификатор» есть также и в матрице с рейтингами. И соответственно
    оставим только эти строчки в матрице с фильмами. Ну и отлично, теперь сделаем переобозначение. Значит, сначала в
    рейтингах меняем ID пользователя, Encoder enc_user.transform из исходных значений, [БЕЗ_ЗВУКА] теперь по фильмам и
    делаем то же самое переобозначение в таблице с описанием фильмов. [БЕЗ_ЗВУКА] Отлично. Первый шаг сделан. Теперь
    давайте создадим саму матрицу. Делать мы это будем с помощью разреженных матриц. Есть много разных форматов
    разреженных матриц, мы будем использовать так называемый координатный формат, то есть для того чтобы ее
    инициализировать, нужно указывать значения и координаты у матрицы по столбцам и строкам, где это значение должно
    храниться. Назовем ее R, конструировать будем вот этим вот конструктором coo_matrix. Делать это будем так: берем
    матрицу рейтингов, достаем оттуда рейтинги и дальше в скобках указываем позицию в этой матрице. Для этого мы сначала
    по строкам пользователи, и затем фильм. [БЕЗ_ЗВУКА] Получили такую разреженную матрицу размера количества п
    ользователей на количество уникальных фильмов. Теперь давайте применим к этой матрице сингулярное разложение.
    На разреженных матрицах есть специальный алгоритм svds, он лежит у нас в библиотеке SciPy. Выход абсолютно такой же,
    как и в обычном сингулярном разложении на плотной матрице. svds, на вход подаем матрицу R и количество компонент
    укажем, например, шесть. Проверим, что мы получили. Матрица u имеет вот такой размер, s — это у нас опять вектор из
    сингулярных чисел, и vt (v транспонированное) — это шесть на количество уникальных фильмов, то есть матрица v — это
    некоторое сжатое признаковое пространство над фильмами. Давайте вот в этом сжатом признаковом пространстве для
    каждого фильма найдем десять его ближайших соседей и посмотрим, как это соотносится с реальной жизнью. Для того
    чтобы найти ближайших соседей, я буду использовать класс NearestNeighbors. Я укажу количество ближайших соседей,
    равное десяти. Все остальное оставлю без изменений. Чтобы получить индексы ближайших соседей, я воспользуюсь методом
    <b>kneighbors</b>. На вход я отправлю матрицу v обычное. Его надо тоже ввести, для этого я v транспонирую, чтобы
    получить нормальное v. И количество ближайших соседей установлю 10. Обучаем алгоритм и находим ближайших соседей. В
    данном случае расстояние нас не будет интересовать, поэтому мы их возвращать, запоминать не будем. Отлично. Теперь в
    матрице ind содержатся индексы десяти ближайших соседей для каждой точки. Естественно, каждый фильм будет ближайший
    самому себе, поэтому первый столбец у нас такой особенный. Теперь для интерпретации я хочу достать название этих
    фильмов, чтобы понять, действительно ли они такие близкие и действительно ли это соответствует реальной жизни.
    Фильмы я сохраню в переменную, которую назову movie_titles. Для этого я, во-первых, из исходной таблички df_movies
    выполню сортировку по идентификатору, уже преобразованному. И достану оттуда столбец с title в виде вектора. И опять
    же, для удобства я опять все запихну в датафрейм. В датафрейме у нас будут следующие колонки. Во-первых, это movie,
    а дальше просто пронумерованные индексы соседей. [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] Вызываем конструктор для датафрейма, на
    вход ему подаем следующую конструкцию: data, это будет movie_titles с индексами, и columns у нас будет cols. То есть
    таким образом, датафрейм будет инициализироваться по матрице со строками с названиями фильмов. Ну и давайте
    посмотрим. Возьмем первый фильм «Историю игрушек», Toy story, и все похожие на него ближайшие соседи это
    действительно какие-то мультики, то есть вроде как похоже на правду. Можно посмотреть, что будет с фильмом
    «Терминатор». Для этого мы проверим поле movie на то, что оно содержит в себе слово Terminator. И получим результат.
    Вот, и здесь на второго «Терминатора» самым похожим является первый, первому — самый похожий второй. Дальше у нас
    идут разные боевики и прочее — вроде как похоже на правду. А у третьего «Терминатора» все немного не так,
    действительно, потому что фильм был оценен по-разному, нежели первый или второй. Отлично. Значит, мы научились
    работать в сжатом признаковом пространстве для фильмов. Теперь давайте научимся считать похожесть между
    пользователями по матрице рейтингов. Конечно, можно схитрить и из sklearn достать метод, который считает косинусное
    расстояние по заданной матрице. Сделает это он довольно быстро. В итоге мы получим матрицу попарных косинусных
    схожеств. Размером этой матрицы будет количество пользователей на количество пользователей. Но с видео мы помним,
    что похожесть можно считать, только используя те фильмы, которые пользователь u и пользователь v посмотрели вместе.
    То есть нам нужно как-то скорректировать эту меру схожести. Сделать это можно следующим образом. Мы напишем функцию
    similarity, которая на вход будет принимать рейтинги пользователя u или рейтинг пользователя v. Первым делом она
    будет находить пересечение между ними, то есть индексы там, где вектор u и вектор v имеют ненулевые значения. Дальше
    если хоть какое-то пересечение есть, то мы посчитаем схожесть как косинус между соответствующими векторами. Стоит
    отметить, что это косинусное расстояние. Для того чтобы получить схожесть, нужно взять минус и добавить +1. А если
    пересечения нет, то мы будем возвращать ноль. Теперь для того чтобы посчитать такое вот наше кастомное косинусное
    расстояние, можно использовать метод pdist. На вход он принимает матрицу, по которой нужно считать схожесть, и в
    качестве метрики вместо какой-то строки с настоящей метрикой мы используем нашу функцию, которую только что
    имплементировали. И ограничение такое, что на вход должна поступать именно плотная матрица, поэтому здесь я
    использую метод toarray. Придется немного подождать. Вычисления закончились. Посмотрим, что лежит в переменной d.
    Это такой длинный вектор. Но для того чтобы перевести его в привычный вид, мы его прогоним через метод
    <b>squareform</b>. И вот на выходе у нас уже та же самая матрица попарных косинусных похожестей между объектами.
    Таким образом, мы посчитали схожесть между объектами по матрице рейтингов, используя кастомное косинусное
    расстояние.
</p></div>

</body>
</html>